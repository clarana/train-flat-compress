# [Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models](https://arxiv.org/abs/2205.12694)

This repository (*currently a work in progress*) will contain modules, notebooks for visualizations, and instructions for replicating and extending experiments featured in our paper. 


**Citation:**
```
@inproceedings{na-etal-2022-trainflat,
    author = {Clara Na and Sanket Vaibhav Mehta and Emma Strubell},
    title = {{Train Flat, Then Compress: Sharpness-Aware Minimization Learns More Compressible Models}},
    booktitle = {Findings of the Conference on Empirical Methods in Natural Language Processing},
    year = {2022},
    url = {https://arxiv.org/abs/2205.12694},
} 
```